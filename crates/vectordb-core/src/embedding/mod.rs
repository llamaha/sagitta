//! Embedding generation and handling logic.

// Use error types from within vectordb_core
use crate::error::{VectorDBError, Result};
use std::fmt;
use crate::config::AppConfig; // Use config from core
use crate::config::load_config;
use crate::config::get_vocabulary_path;

// Provider related imports will need adjustment
#[cfg(feature = "ort")]
// use crate::embedding::provider::onnx::OnnxEmbeddingModel; // Unused
use self::provider::EmbeddingProvider; // Use internal provider trait

// These need dependencies in vectordb_core
use std::sync::{Arc, Mutex}; // Removed unused Mutex, MutexGuard
use std::path::{Path, PathBuf};
use serde::{Serialize, Deserialize};
use log;

// Keep these imports - check dependencies later

/// Enum representing the type of embedding model to use.
#[derive(Default, Debug, Clone, Serialize, Deserialize, PartialEq, Eq, Hash)]
pub enum EmbeddingModelType {
    #[default]
    Default, // Add a default variant if needed, or choose one
    Onnx,    // Keep Onnx
    // Add other types later
}

impl EmbeddingModelType {
    /// Get the expected dimension for the model type.
    pub fn dimension(&self) -> usize {
        match self {
            EmbeddingModelType::Onnx => 384,
            EmbeddingModelType::Default => 384, // Use default dimension for now
            // Add other model types here
        }
    }
}

impl fmt::Display for EmbeddingModelType {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        match self {
            EmbeddingModelType::Onnx => write!(f, "ONNX"),
            EmbeddingModelType::Default => write!(f, "Default"),
        }
    }
}

/// Represents an embedding model (currently ONNX-based).
#[derive(Clone, Debug)] // Removed derive(Serialize, Deserialize) if not needed
pub struct EmbeddingModel {
    provider: Arc<dyn EmbeddingProvider + Send + Sync>,
    model_type: EmbeddingModelType,
    #[allow(dead_code)] // Allow dead code for now, may be used for identification/debugging
    onnx_model_path: Option<PathBuf>,
    #[allow(dead_code)] // Allow dead code for now, may be used for identification/debugging
    onnx_tokenizer_path: Option<PathBuf>,
}

impl EmbeddingModel {
    /// Creates a new ONNX-based EmbeddingModel.
    #[cfg(feature = "ort")]
    pub fn new_onnx<P: AsRef<Path>>(model_path: P, tokenizer_path: P) -> Result<Self> {
        // Use provider from crate::embedding::provider::onnx
        let onnx_provider = crate::embedding::provider::onnx::OnnxEmbeddingModel::new(
            model_path.as_ref(), 
            tokenizer_path.as_ref()
        ).map_err(|e| VectorDBError::EmbeddingError(format!("Failed to create ONNX provider: {}", e)))?; // Explicitly map error
        
        Ok(Self {
            provider: Arc::new(onnx_provider),
            model_type: EmbeddingModelType::Onnx,
            onnx_model_path: Some(model_path.as_ref().to_path_buf()),
            onnx_tokenizer_path: Some(tokenizer_path.as_ref().to_path_buf()),
        })
    }

    /// Get the type of the embedding model.
    pub fn get_type(&self) -> EmbeddingModelType {
        self.model_type.clone()
    }

    /// Get the dimensions of the embeddings generated by this model.
    pub fn dim(&self) -> usize {
        let provider_ref: &dyn EmbeddingProvider = self.provider.as_ref();
        provider_ref.dimension()
    }

    /// Generates embeddings for a batch of texts.
    pub fn embed_batch(&self, texts: &[&str]) -> Result<Vec<Vec<f32>>> {
        let provider_ref: &dyn EmbeddingProvider = self.provider.as_ref();
        provider_ref.embed_batch(texts).map_err(Into::into)
    }
}


// --- Placeholder Modules ---
pub mod types;
pub mod provider;

// Function previously in embedding_logic.rs
#[derive(Debug)]
pub struct EmbeddingHandler {
    pub embedding_model_type: EmbeddingModelType,
    pub onnx_model_path: Option<PathBuf>,
    pub onnx_tokenizer_path: Option<PathBuf>,
}

impl EmbeddingHandler {
    pub fn new(config: &AppConfig) -> std::result::Result<Self, VectorDBError> {
        let model_type = EmbeddingModelType::Onnx; // Assume ONNX for now
        Ok(Self {
            embedding_model_type: model_type,
            onnx_model_path: config.onnx_model_path.clone().map(PathBuf::from),
            onnx_tokenizer_path: config.onnx_tokenizer_path.clone().map(PathBuf::from),
        })
    }

    pub fn dimension(&self) -> Result<usize> {
        let model_path = self.onnx_model_path.as_ref().ok_or_else(|| {
            VectorDBError::EmbeddingError("ONNX model path not set in handler.".to_string())
        })?;
        let tokenizer_path = self.onnx_tokenizer_path.as_ref().ok_or_else(|| {
            VectorDBError::EmbeddingError("ONNX tokenizer path not set in handler.".to_string())
        })?;
        let model = crate::embedding::provider::onnx::OnnxEmbeddingModel::new(model_path, tokenizer_path)
            .map_err(VectorDBError::from)?;
        Ok(model.dimension())
    }

    /// Generates embeddings for a batch of texts using a fresh model per call.
    pub fn embed(&self, texts: &[&str]) -> Result<Vec<Vec<f32>>> {
        let model_path = self.onnx_model_path.as_ref().ok_or_else(|| {
            VectorDBError::EmbeddingError("ONNX model path not set in handler.".to_string())
        })?;
        let tokenizer_path = self.onnx_tokenizer_path.as_ref().ok_or_else(|| {
            VectorDBError::EmbeddingError("ONNX tokenizer path not set in handler.".to_string())
        })?;
        let model = crate::embedding::provider::onnx::OnnxEmbeddingModel::new(model_path, tokenizer_path)
            .map_err(VectorDBError::from)?;
        model.embed_batch(texts).map_err(VectorDBError::from)
    }
}

// --- Tests --- 
#[cfg(test)]
mod tests {
    use super::*;
    use crate::config::AppConfig;
    use crate::config::IndexingConfig;
    use crate::embedding::{EmbeddingHandler, EmbeddingModelType, EmbeddingModel};
    use crate::error::VectorDBError;
    use std::fs;
    use tempfile::tempdir;

    fn create_test_config() -> AppConfig {
        let temp_dir = tempdir().unwrap();
        let model_base = temp_dir.path().join("models");
        let vocab_base = temp_dir.path().join("vocab");
        fs::create_dir_all(&model_base).unwrap();
        fs::create_dir_all(&vocab_base).unwrap();

        // Create dummy model/tokenizer files needed by the handler
        let dummy_model = model_base.join("model.onnx");
        let dummy_tokenizer_dir = model_base.join("tokenizer");
        let dummy_tokenizer_file = dummy_tokenizer_dir.join("tokenizer.json");
        fs::write(&dummy_model, "dummy model data").unwrap();
        fs::create_dir(&dummy_tokenizer_dir).unwrap();
        fs::write(&dummy_tokenizer_file, "{}").unwrap(); // Minimal valid JSON

        AppConfig {
            repositories: vec![],
            active_repository: None,
            qdrant_url: "http://localhost:6333".to_string(),
            onnx_model_path: Some(dummy_model.to_string_lossy().into_owned()), // Correct field name
            onnx_tokenizer_path: Some(dummy_tokenizer_dir.to_string_lossy().into_owned()), // Correct field name
            server_api_key_path: None, // Correct field name
            repositories_base_path: None,
            vocabulary_base_path: Some(vocab_base.to_string_lossy().into_owned()),
            indexing: Default::default(),
            performance: Default::default(),
        }
    }

    #[cfg(feature = "ort")] // Only run ONNX tests if feature is enabled
    #[test]
    fn test_embedding_handler_new_onnx_valid_paths() {
        let dir = tempdir().unwrap();
        let model_path = dir.path().join("model.onnx");
        let tokenizer_dir = dir.path().join("tokenizer");
        fs::create_dir(&tokenizer_dir).unwrap();
        let tokenizer_path = tokenizer_dir.join("tokenizer.json");
        fs::write(&model_path, b"dummy").unwrap();
        // Minimal valid tokenizer JSON
        fs::write(&tokenizer_path, r#"{"model": {"vocab": {}}}"#).unwrap(); 

        let config = create_test_config();
        
        // This might still fail if ORT libs aren't found or the dummy model is invalid
        let result = EmbeddingHandler::new(&config);
        if result.is_err() {
            println!("Note: Handler creation failed, likely due to ORT setup or dummy model: {:?}", result.err());
        }
        // For basic testing, we often just check if it doesn't panic and returns Result
        // assert!(result.is_ok()); // This might be too strict depending on ORT env
    }

    #[test]
    fn test_embedding_handler_new_onnx_missing_paths() {
        // Create a config *without* setting the ONNX paths
        let mut config = AppConfig::default();
        config.onnx_model_path = None;
        config.onnx_tokenizer_path = None;

        let result = EmbeddingHandler::new(&config); 
        // Now, this assertion should correctly expect a ConfigurationError
        assert!(matches!(result, Err(VectorDBError::ConfigurationError(_))), "Expected ConfigurationError when ONNX paths are None, but got: {:?}", result);
    }

    #[cfg(feature = "ort")]
    #[test]
    fn test_embedding_model_new_onnx_invalid_path() {
        // Test for EmbeddingModel struct specifically, if it's still used/needed
        let model_path = PathBuf::from("./nonexistent/model.onnx");
        let tokenizer_path = PathBuf::from("./nonexistent/tokenizer.json");
        let result = EmbeddingModel::new_onnx(&model_path, &tokenizer_path);
        assert!(matches!(result, Err(VectorDBError::EmbeddingError(_))));
    }

    #[test]
    fn test_embedding_model_type_display() {
        assert_eq!(EmbeddingModelType::Onnx.to_string(), "ONNX");
        assert_eq!(EmbeddingModelType::Default.to_string(), "Default");
    }

    // Added test from src/vectordb/embedding.rs
    #[test]
    fn test_onnx_embedding_fallback() {
        // Use the core constructor directly
        // The ONNX feature needs to be enabled on the core crate
        #[cfg(feature = "ort")] // Keep cfg gate if applicable
        {
            let model_path = Path::new("onnx/all-minilm-l12-v2.onnx");
            let tokenizer_path = Path::new("onnx/minilm_tokenizer.json");

            // Skip test if ONNX files don't exist
            if !model_path.exists() || !tokenizer_path.exists() {
                println!("Skipping ONNX test because model files aren't available");
                return;
            }

            // Create ONNX model using the core's constructor
            // Note: This assumes the 'ort' feature is enabled for vectordb-core
            let onnx_model_result = EmbeddingModel::new_onnx(model_path, tokenizer_path);
            assert!(onnx_model_result.is_ok());

            let model = onnx_model_result.unwrap();
            let expected_dim = model.dim(); // Get dimension from model

            // Test embedding
            let text = "fn main() { let x = 42; }";
            let embedding_result = model.embed_batch(&[text]);
            assert!(embedding_result.is_ok());
            let embedding = embedding_result.unwrap().into_iter().next().unwrap();

            assert_eq!(embedding.len(), expected_dim); // Check against model's dimension
            assert!(!embedding.iter().all(|&x| x == 0.0));

            // Test cloning
            let cloned_model = model.clone();
            assert_eq!(cloned_model.dim(), expected_dim);
            let cloned_embedding_result = cloned_model.embed_batch(&[text]);
            assert!(cloned_embedding_result.is_ok());
            let cloned_embedding = cloned_embedding_result.unwrap().into_iter().next().unwrap();
            assert_eq!(embedding, cloned_embedding);
        }
        #[cfg(not(feature = "ort"))]
        {
            println!("Skipping ONNX test because 'ort' feature is not enabled for vectordb-core");
        }
    }

    #[cfg(feature = "ort")]
    fn test_onnx_embedding_model_creation_and_use() {
        // Added test from src/vectordb/embedding.rs

        // Construct path relative to crate root, assuming models are in workspace_root/onnx/
        let manifest_dir = PathBuf::from(env!("CARGO_MANIFEST_DIR"));
        let workspace_root = manifest_dir.parent().expect("Failed to get parent dir of manifest dir"); // Go up to workspace root
        let default_model_path = workspace_root.join("onnx/all-minilm-l6-v2.onnx");
        let default_tokenizer_path = workspace_root.join("onnx/tokenizer.json");

        let model_path = std::env::var("VECTORDB_ONNX_MODEL")
            .map(PathBuf::from)
            .unwrap_or(default_model_path);
        let tokenizer_path = std::env::var("VECTORDB_ONNX_TOKENIZER")
            .map(PathBuf::from)
            .unwrap_or(default_tokenizer_path);

        // Debug print the paths
        println!("Using Model Path: {:?}", model_path.canonicalize().unwrap_or_else(|_| model_path.clone()));
        println!("Using Tokenizer Path: {:?}", tokenizer_path.canonicalize().unwrap_or_else(|_| tokenizer_path.clone()));

        let embedding_model = EmbeddingModel::new_onnx(model_path, tokenizer_path)
            .expect("Failed to create ONNX EmbeddingModel in test");

        let expected_dim = embedding_model.dim(); // Get dimension from model

        // Test embedding
        let text = "fn main() { let x = 42; }";
        let embedding_result = embedding_model.embed_batch(&[text]);
        assert!(embedding_result.is_ok());
        let embedding = embedding_result.unwrap().into_iter().next().unwrap();

        assert_eq!(embedding.len(), expected_dim); // Check against model's dimension
        assert!(!embedding.iter().all(|&x| x == 0.0));

        // Test cloning
        let cloned_model = embedding_model.clone();
        assert_eq!(cloned_model.dim(), expected_dim);
        let cloned_embedding_result = cloned_model.embed_batch(&[text]);
        assert!(cloned_embedding_result.is_ok());
        let cloned_embedding = cloned_embedding_result.unwrap().into_iter().next().unwrap();
        assert_eq!(embedding, cloned_embedding);
    }

    #[test]
    #[should_panic] // Expect panic because dummy model path doesn't exist
    fn test_handler_creation_fail_onnx() {
        if !cfg!(feature = "onnx") {
            // Removed: println!("Skipping ONNX test because 'onnx' feature is not enabled for vectordb-core");
            panic!(); // Fail test if feature disabled
        }
        let config = AppConfig {
            // Assuming AppConfig fields are here, ending with ..Default::default()
            performance: Default::default(),
            ..Default::default()
        };
        let result = EmbeddingHandler::new(&config);
        if result.is_err() {
            // Removed: println!("Note: Handler creation failed, likely due to ORT setup or dummy model: {:?}", result.err());
            // If ONNX is enabled but fails (e.g., bad path, ORT issue), this is expected here.
            panic!(); // Trigger the should_panic
        }
        // Code that might have been intended after the 'if result.is_err()' block
        // If the test should succeed if result.is_ok(), no panic is needed here.
        // If the test should *always* panic (because the setup is expected to fail),
        // we might need to add a panic here if result.is_ok().
        // Let's assume for now the test just ensures it panics on error.
    }

    #[test]
    #[should_panic] // Expect panic if model files are missing
    fn test_handler_creation_with_real_paths_but_no_files() {
        if !cfg!(feature = "onnx") {
            // Removed: println!("Skipping ONNX test because model files aren't available");
            panic!(); // Ensure test fails if feature disabled
        }
        // Configuration code that would lead to panic if files don't exist
        let config = AppConfig {
            // Set paths that *should* exist but we ensure they don't
            onnx_model_path: Some("./nonexistent/model.onnx".to_string()),
            onnx_tokenizer_path: Some("./nonexistent/tokenizer".to_string()),
            performance: Default::default(),
            ..Default::default()
        };
        let _ = EmbeddingHandler::new(&config); // This should panic
    }

    #[test]
    #[ignore] // Needs actual model files and ONNX runtime
    fn test_onnx_embedding() -> crate::error::Result<()> {
        if !cfg!(feature = "onnx") {
            // Removed: println!("Skipping ONNX test because 'ort' feature is not enabled for vectordb-core");
            return Ok(());
        }
        // Assume model paths are correctly set via environment or config for this test
        let config = load_config(None)?;
        let collection_name = "repo_test";
        let _default_path = get_vocabulary_path(&config, collection_name)?;
        let custom_config = config.clone();
        let _custom_path = get_vocabulary_path(&custom_config, collection_name)?;
        let handler = EmbeddingHandler::new(&config)?;

        // Removed: println!("Using Model Path: {:?}", config.onnx_model_path.as_ref().unwrap());
        // Removed: println!("Using Tokenizer Path: {:?}", config.onnx_tokenizer_path.as_ref().unwrap());

        let texts = ["this is a test"];
        let embeddings = handler.embed(&texts)?;
        assert_eq!(embeddings.len(), 1);
        // Placeholder assert - check dimension or non-zero values if possible
        assert!(!embeddings[0].is_empty());
        Ok(())
    }
} 