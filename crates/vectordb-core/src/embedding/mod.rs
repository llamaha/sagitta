// Contents moved from src/vectordb/embedding.rs

// Use crate::embedding::types for types eventually
// use crate::embedding::types::{EmbeddingModel, Feature}; // Placeholder
// Use error types from within vectordb_core
use crate::error::{Result, VectorDBError};
use std::fmt;
use crate::config::AppConfig; // Use config from core

// Provider related imports will need adjustment
#[cfg(feature = "ort")]
// use crate::embedding::provider::onnx::OnnxEmbeddingModel; // Unused
use crate::embedding::provider::EmbeddingProvider; // Adjust path

// These need dependencies in vectordb-core
use std::sync::{Arc, Mutex};
use std::path::{Path, PathBuf};
use serde::{Serialize, Deserialize};
use log;

// Keep these imports - check dependencies later
#[cfg(feature = "ort")]
use {
    // tokenizers::Tokenizer, // Unused
    // ndarray::{Axis, Array2}, // Unused
};

/// Enum representing the type of embedding model to use.
#[derive(Clone, Copy, Debug, PartialEq, Eq, Serialize, Deserialize, Default)]
pub enum EmbeddingModelType {
    #[default]
    Default, // Represents the default model
    Onnx,
    // Add other model types here in the future (e.g., SentenceTransformers)
}

impl EmbeddingModelType {
    /// Get the expected dimension for the model type.
    pub fn dimension(&self) -> usize {
        match self {
            EmbeddingModelType::Onnx => 384,
            EmbeddingModelType::Default => 384, // Use default dimension for now
            // Add other model types here
        }
    }
}

impl fmt::Display for EmbeddingModelType {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        match self {
            EmbeddingModelType::Onnx => write!(f, "ONNX"),
            EmbeddingModelType::Default => write!(f, "Default"),
        }
    }
}

/// Represents an embedding model (currently ONNX-based).
#[derive(Clone, Debug)] // Removed derive(Serialize, Deserialize) if not needed
pub struct EmbeddingModel {
    provider: Arc<dyn EmbeddingProvider + Send + Sync>,
    model_type: EmbeddingModelType,
    onnx_model_path: Option<PathBuf>,
    onnx_tokenizer_path: Option<PathBuf>,
}

impl EmbeddingModel {
    /// Creates a new ONNX-based EmbeddingModel.
    #[cfg(feature = "ort")]
    pub fn new_onnx<P: AsRef<Path>>(model_path: P, tokenizer_path: P) -> Result<Self> {
        // Use provider from crate::embedding::provider::onnx
        let onnx_provider = crate::embedding::provider::onnx::OnnxEmbeddingModel::new(
            model_path.as_ref(), 
            tokenizer_path.as_ref()
        ).map_err(|e| VectorDBError::EmbeddingError(format!("Failed to create ONNX provider: {}", e)))?; // Explicitly map error
        
        Ok(Self {
            provider: Arc::new(onnx_provider),
            model_type: EmbeddingModelType::Onnx,
            onnx_model_path: Some(model_path.as_ref().to_path_buf()),
            onnx_tokenizer_path: Some(tokenizer_path.as_ref().to_path_buf()),
        })
    }

    /// Get the type of the embedding model.
    pub fn model_type(&self) -> EmbeddingModelType {
        self.model_type
    }

    /// Get the dimensions of the embeddings generated by this model.
    pub fn dim(&self) -> usize {
        let provider_ref: &dyn EmbeddingProvider = self.provider.as_ref();
        provider_ref.dimension()
    }

    /// Generates embeddings for a batch of texts.
    pub fn embed_batch(&self, texts: &[&str]) -> Result<Vec<Vec<f32>>> {
        let provider_ref: &dyn EmbeddingProvider = self.provider.as_ref();
        provider_ref.embed_batch(texts).map_err(Into::into)
    }
}


// --- Placeholder Modules ---
pub mod types;
pub mod provider;

// Function previously in embedding_logic.rs
#[derive(Debug)] // Added Debug derive
pub struct EmbeddingHandler {
    embedding_model_type: EmbeddingModelType,
    onnx_model_path: Option<PathBuf>,
    onnx_tokenizer_path: Option<PathBuf>,
    provider_cache: Mutex<Option<Box<dyn EmbeddingProvider>>>,
}

impl EmbeddingHandler {
    pub fn new(config: &AppConfig) -> std::result::Result<Self, VectorDBError> {
        let model_type = EmbeddingModelType::Onnx; // Assume ONNX for now
        
        let provider_result: Result<Box<dyn EmbeddingProvider>> = match model_type {
            EmbeddingModelType::Onnx | EmbeddingModelType::Default => {
                #[cfg(feature = "ort")]
                {
                    let model_path_str = config.onnx_model_path.as_deref()
                        .ok_or_else(|| VectorDBError::ConfigurationError("ONNX model path not set in AppConfig".to_string()))?;
                    let tokenizer_path_str = config.onnx_tokenizer_path.as_deref()
                        .ok_or_else(|| VectorDBError::ConfigurationError("ONNX tokenizer path not set in AppConfig".to_string()))?;
                    
                    let model_path = PathBuf::from(model_path_str);
                    let tokenizer_path = PathBuf::from(tokenizer_path_str);

                    provider::onnx::OnnxEmbeddingModel::new(&model_path, &tokenizer_path)
                        .map(|provider| Box::new(provider) as Box<dyn EmbeddingProvider>)
                        .map_err(VectorDBError::from) // Map anyhow::Error
                }
                #[cfg(not(feature = "ort"))]
                {
                     Err(VectorDBError::FeatureNotEnabled("ort".to_string()))
                }
            },
        };

        Ok(Self {
            embedding_model_type: model_type,
            onnx_model_path: config.onnx_model_path.clone().map(PathBuf::from),
            onnx_tokenizer_path: config.onnx_tokenizer_path.clone().map(PathBuf::from),
            provider_cache: Mutex::new(provider_result.ok()),
        })
    }

    #[cfg(feature="ort")]
    pub fn create_embedding_model(&self) -> Result<Box<dyn EmbeddingProvider>> {
        match self.embedding_model_type {
            EmbeddingModelType::Onnx => {
                let model_path = self.onnx_model_path.as_ref().ok_or_else(|| {
                    VectorDBError::EmbeddingError("ONNX model path not set in handler.".to_string())
                })?;
                let tokenizer_path = self.onnx_tokenizer_path.as_ref().ok_or_else(|| {
                    VectorDBError::EmbeddingError("ONNX tokenizer path not set in handler.".to_string())
                })?;
                let provider: Box<dyn EmbeddingProvider> = Box::new(provider::onnx::OnnxEmbeddingModel::new(
                    model_path,
                    tokenizer_path,
                )?);
                Ok(provider)
            }
            EmbeddingModelType::Default => {
                 Err(VectorDBError::NotImplemented("Default embedding model provider not yet implemented".to_string()))
            }
        }
    }

    #[cfg(not(feature="ort"))]
    pub fn create_embedding_model(&self) -> Result<Box<dyn EmbeddingProvider>> {
        Err(VectorDBError::FeatureNotEnabled("ort".to_string()))
    }

    pub fn set_onnx_paths(
        &mut self,
        model_path: Option<PathBuf>,
        tokenizer_path: Option<PathBuf>,
    ) -> Result<()> {
        if let Some(model_p) = &model_path {
            if !model_p.exists() {
                return Err(VectorDBError::EmbeddingError(format!(
                    "ONNX model file not found: {}",
                    model_p.display()
                )));
            }
        }
        // Removed check for tokenizer path existence as it might be a directory

        if model_path.is_some() || tokenizer_path.is_some() {
            self.embedding_model_type = EmbeddingModelType::Onnx;
        }

        self.onnx_model_path = model_path;
        self.onnx_tokenizer_path = tokenizer_path;
        self.provider_cache.lock().unwrap().take();
        Ok(())
    }

    pub fn embedding_model_type(&self) -> EmbeddingModelType {
        self.embedding_model_type
    }

    pub fn onnx_model_path(&self) -> Option<&PathBuf> {
        self.onnx_model_path.as_ref()
    }

    pub fn onnx_tokenizer_path(&self) -> Option<&PathBuf> {
        self.onnx_tokenizer_path.as_ref()
    }

    pub fn dimension(&self) -> Result<usize> {
        let mut cache_guard = self.provider_cache.lock().unwrap();
        if cache_guard.is_none() {
            log::debug!("Provider cache miss for dimension. Creating provider...");
            let provider = self.create_embedding_model()?;
            cache_guard.replace(provider);
        }
        cache_guard.as_ref().map(|p| p.dimension()).ok_or_else(|| VectorDBError::EmbeddingError("Failed to get provider dimension".to_string()))
    }

    pub fn embed(&self, texts: &[&str]) -> Result<Vec<Vec<f32>>> {
        let mut cache_guard = self.provider_cache.lock().unwrap();
        if cache_guard.is_none() {
            log::debug!("Provider cache miss for embed. Creating provider...");
            let provider = self.create_embedding_model()?;
            cache_guard.replace(provider);
        }
        cache_guard.as_mut()
            .ok_or_else(|| VectorDBError::EmbeddingError("Failed to get provider for embedding".to_string()))?
            .embed_batch(texts)
    }
}

// --- Tests --- 
#[cfg(test)]
mod tests {
    use super::*;
    use crate::config::AppConfig;
    use crate::error::VectorDBError;
    use std::fs;
    use tempfile::tempdir;

    fn create_test_config(
        model_path: Option<String>,
        tokenizer_path: Option<String>,
    ) -> AppConfig {
        AppConfig {
            qdrant_url: "http://localhost:6334".to_string(),
            onnx_model_path: model_path,
            onnx_tokenizer_path: tokenizer_path,
            server_api_key_path: None,
            repositories: vec![],
            active_repository: None,
            repositories_base_path: None,
        }
    }

    #[cfg(feature = "ort")] // Only run ONNX tests if feature is enabled
    #[test]
    fn test_embedding_handler_new_onnx_valid_paths() {
        let dir = tempdir().unwrap();
        let model_path = dir.path().join("model.onnx");
        let tokenizer_dir = dir.path().join("tokenizer");
        fs::create_dir(&tokenizer_dir).unwrap();
        let tokenizer_path = tokenizer_dir.join("tokenizer.json");
        fs::write(&model_path, b"dummy").unwrap();
        // Minimal valid tokenizer JSON
        fs::write(&tokenizer_path, r#"{"model": {"vocab": {}}}"#).unwrap(); 

        let config = create_test_config(
            Some(model_path.to_str().unwrap().to_string()),
            Some(tokenizer_dir.to_str().unwrap().to_string()), // Pass dir path
        );
        
        // This might still fail if ORT libs aren't found or the dummy model is invalid
        let result = EmbeddingHandler::new(&config);
        if result.is_err() {
            println!("Note: Handler creation failed, likely due to ORT setup or dummy model: {:?}", result.err());
        }
        // For basic testing, we often just check if it doesn't panic and returns Result
        // assert!(result.is_ok()); // This might be too strict depending on ORT env
    }

    #[test]
    fn test_embedding_handler_new_onnx_missing_paths() {
        let config = create_test_config(None, None); 
        let result = EmbeddingHandler::new(&config);
        assert!(matches!(result, Err(VectorDBError::ConfigurationError(_))));
    }

    #[cfg(feature = "ort")]
    #[test]
    fn test_embedding_model_new_onnx_invalid_path() {
        // Test for EmbeddingModel struct specifically, if it's still used/needed
        let model_path = PathBuf::from("./nonexistent/model.onnx");
        let tokenizer_path = PathBuf::from("./nonexistent/tokenizer.json");
        let result = EmbeddingModel::new_onnx(&model_path, &tokenizer_path);
        assert!(matches!(result, Err(VectorDBError::EmbeddingError(_))));
    }

    #[test]
    fn test_embedding_model_type_display() {
        assert_eq!(EmbeddingModelType::Onnx.to_string(), "ONNX");
        assert_eq!(EmbeddingModelType::Default.to_string(), "Default");
    }
} 