use crate::vectordb::error::VectorDBError;
use crate::vectordb::provider::onnx::ONNX_EMBEDDING_DIM;
use crate::vectordb::provider::{EmbeddingProvider, OnnxEmbeddingProvider};
use anyhow::Result;
use serde::{Deserialize, Serialize};
use std::path::Path;

// Use the embedding dimensions from the providers
// use crate::vectordb::provider::fast::FAST_EMBEDDING_DIM;

/// Dimension of the embeddings generated by the model.
// We may need to make this dynamic based on the model loaded?
pub const EMBEDDING_DIM: usize = ONNX_EMBEDDING_DIM;

/// Supported embedding models.
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
pub enum EmbeddingModelType {
    /// Use the ONNX model for embeddings.
    Onnx,
}

impl std::fmt::Display for EmbeddingModelType {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            EmbeddingModelType::Onnx => write!(f, "ONNX"),
        }
    }
}

impl Default for EmbeddingModelType {
    fn default() -> Self {
        EmbeddingModelType::Onnx
    }
}

/// Model for generating embeddings from text
pub struct EmbeddingModel {
    provider: Box<dyn EmbeddingProvider + Send + Sync>,
    model_type: EmbeddingModelType,
}

impl Clone for EmbeddingModel {
    fn clone(&self) -> Self {
        // Cloning is tricky as the provider has state (e.g., ONNX session).
        // Re-creating the provider is the safest approach.
        // This assumes the necessary paths are available via self.db or environment.
        // For simplicity here, we panic if trying to clone without necessary info,
        // but a real implementation might need access to the DB's path storage.
        match self.model_type {
            EmbeddingModelType::Onnx => {
                // Ideally, we'd get paths from where the original was created.
                // Since we don't store them here, we rely on environment vars as fallback.
                let model_path = std::env::var("VECTORDB_ONNX_MODEL")
                    .expect("VECTORDB_ONNX_MODEL env var needed for cloning ONNX model");
                let tokenizer_path = std::env::var("VECTORDB_ONNX_TOKENIZER")
                    .expect("VECTORDB_ONNX_TOKENIZER env var needed for cloning ONNX model");
                
                Self::new_onnx(Path::new(&model_path), Path::new(&tokenizer_path))
                    .expect("Failed to re-create ONNX model during clone")
            }
        }
    }
}

impl EmbeddingModel {
    /// Creates a new EmbeddingModel with the Fast provider
    /// This provider is much faster but less accurate than ONNX
    // pub fn new() -> Self {
    //     let provider = Box::new(FastTextProvider::new());
    //     Self {
    //         provider,
    //         model_type: EmbeddingModelType::Fast,
    //     }
    // }

    /// Creates a new EmbeddingModel with the ONNX provider
    /// This provider is more accurate but slower than Fast
    pub fn new_onnx(model_path: &Path, tokenizer_path: &Path) -> Result<Self> {
        let provider = Box::new(OnnxEmbeddingProvider::new(model_path, tokenizer_path)?);

        Ok(Self {
            provider,
            model_type: EmbeddingModelType::Onnx,
        })
    }

    /// Convert text to an embedding vector
    pub fn embed(&self, text: &str) -> Result<Vec<f32>, VectorDBError> {
        self.provider
            .embed(text)
            .map_err(|e| VectorDBError::EmbeddingError(e.to_string()))
    }

    /// Convert multiple texts to embedding vectors
    pub fn embed_batch(&self, texts: &[&str]) -> Result<Vec<Vec<f32>>, VectorDBError> {
        self.provider
            .embed_batch(texts)
            .map_err(|e| VectorDBError::EmbeddingError(e.to_string()))
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    // Removed test_fast_embedding
    // #[test]
    // fn test_fast_embedding() { ... }

    // Removed test_embedding_batch (it used the default FastText model)
    // #[test]
    // fn test_embedding_batch() { ... }

    // Keep test_onnx_embedding_fallback
    #[test]
    fn test_onnx_embedding_fallback() {
        let model_path = Path::new("onnx/all-minilm-l12-v2.onnx");
        let tokenizer_path = Path::new("onnx/minilm_tokenizer.json");

        // Skip test if ONNX files don't exist
        if !model_path.exists() || !tokenizer_path.exists() {
            println!("Skipping ONNX test because model files aren't available");
            return;
        }

        // Create ONNX model
        let onnx_model = EmbeddingModel::new_onnx(model_path, tokenizer_path);
        assert!(onnx_model.is_ok());

        let model = onnx_model.unwrap();

        // Test embedding
        let text = "fn main() { let x = 42; }";
        let embedding = model.embed(text).unwrap();

        assert_eq!(embedding.len(), EMBEDDING_DIM);
        assert!(!embedding.iter().all(|&x| x == 0.0));
    }

    // Removed test_model_cloning (it used the default FastText model)
    // #[test]
    // fn test_model_cloning() { ... }
}
